{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "456bd7c62133d64a77fa64d50a41c1557c4e36a8"
   },
   "source": [
    "*2019/03/23 Update*:  Inspired by [hanxiao/bert-as-service\n",
    "](https://github.com/hanxiao/bert-as-service), the hidden states (context vectors) of the second-to-last layer is used instead of the ones from the last layer. \n",
    "\n",
    "> Q: Why not the last hidden layer? Why second-to-last?\n",
    "\n",
    "> A: The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set pooling_layer=-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "904629c60c381a925b60d989d2c2d4dfa5a174f4",
    "colab_type": "text",
    "id": "WRANK2PZo8FX"
   },
   "source": [
    "Changes made:\n",
    "\n",
    "1.  **KFold Validation and Bagging**. Currently only the MLP layers are trained, so a lot of repetitive feed-forwards are incurred between folds. But the current workflow allows you to unfreeze at least some of the layers of Bert encoder when you have the computing resources and big enough dataset.\n",
    "2. **[EndpointSpanExtractor](https://github.com/allenai/allennlp/blob/580dc8b0e2c6491d4d75b54c3b15b34b462e0c67/allennlp/modules/span_extractors/endpoint_span_extractor.py)** is used instead of [SelfAttentiveSpanExtractor](https://github.com/allenai/allennlp/blob/580dc8b0e2c6491d4d75b54c3b15b34b462e0c67/allennlp/modules/span_extractors/self_attentive_span_extractor.py). The self-attention layer did not work so well (probably because of the small size of the dataset). This extractor only use the start and the end of the span. \n",
    "3. Slight modification in Dataset Preparation Code. A small bug is fixed and the label transformation is moved out of the dataset class.\n",
    "4. Add a weight decay wrapper (to implement a variant of [AdamW](https://arxiv.org/pdf/1711.05101.pdf)). Not sure if it helps, but it's there for you to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2434
    },
    "colab_type": "code",
    "id": "zxktT22iAovZ",
    "outputId": "f9a05157-b60c-4ce4-aa11-b400a951c72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  removed specs:\r\n",
      "    - greenlet\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be REMOVED:\r\n",
      "\r\n",
      "  gevent-1.3.0-py36h14c3975_0\r\n",
      "  greenlet-0.4.13-py36h14c3975_0\r\n",
      "\r\n",
      "\r\n",
      "Preparing transaction: | \b\bdone\r\n",
      "Verifying transaction: - \b\bdone\r\n",
      "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "Collecting pytorch-pretrained-bert\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 3.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2018.1.10)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.118)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.2)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.118 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.118)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (2.6.0)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (0.14)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.118->boto3->pytorch-pretrained-bert) (1.12.0)\r\n",
      "Installing collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\r\n",
      "Collecting allennlp\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/32/d6d0a93a23763f366df2dbd4e007e45ce4d2ad97e6315506db9da8af7731/allennlp-0.8.2-py3-none-any.whl (5.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 7.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.21.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.0)\r\n",
      "Collecting responses>=0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.5.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.16.2)\r\n",
      "Collecting awscli>=1.11.91 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/fe/1dfe7e1cf92a80835086d4e6ac6bcedd4543b74626e3085c23233cae4320/awscli-1.16.130-py2.py3-none-any.whl (1.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 15.7MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.2.4)\r\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.23)\r\n",
      "Requirement already satisfied: flask==1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.2)\r\n",
      "Collecting flaky (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.9.118)\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.4.3)\r\n",
      "Collecting overrides (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\r\n",
      "Collecting parsimonious==0.8.0 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.9.0)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.5.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.20.3)\r\n",
      "Requirement already satisfied: spacy<2.1,>=2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.0.18)\r\n",
      "Collecting moto==1.3.4 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl (548kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 21.0MB/s \r\n",
      "\u001b[?25hCollecting tensorboardX==1.2 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 22.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.31.1)\r\n",
      "Collecting matplotlib==2.2.3 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 12.6MB 3.5MB/s \r\n",
      "\u001b[?25hCollecting jsonnet==0.10.0; sys_platform != \"win32\" (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/83/d49904ee98dd4fbba6a003938e30e76251951c4bdb49628b4f92e5009a42/jsonnet-0.10.0.tar.gz (124kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 5.9MB/s \r\n",
      "\u001b[?25hCollecting sqlparse==0.2.4 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/85/20bdd72f4537cf2c4d5d005368d502b2f464ede22982e724a82c86268eda/sqlparse-0.2.4-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.6.1)\r\n",
      "Collecting editdistance (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 26.6MB/s \r\n",
      "\u001b[?25hCollecting flask-cors==3.0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\r\n",
      "Collecting pytz==2017.3 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 21.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.1.post2)\r\n",
      "Collecting conllu==0.11 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\r\n",
      "Collecting gevent==1.3.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/3d/a19fece28ba1b5133cf74bd22a229d77b4d9cc4b24aa8f263cca2845c555/gevent-1.3.6-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 4.5MB 9.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpydoc==0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.8.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (1.22)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from responses>=0.7->allennlp) (1.12.0)\r\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (3.12)\r\n",
      "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.14)\r\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.3.9)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.2.0)\r\n",
      "Collecting botocore==1.12.120 (from awscli>=1.11.91->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/34/f93fdd88dd35e518054ea9f353b6bb4b4bd8721ca24cd63a8adf373d782d/botocore-1.12.120-py2.py3-none-any.whl (5.3MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.3MB 9.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /opt/conda/lib/python3.6/site-packages (from flask==1.0.2->allennlp) (2.10)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask==1.0.2->allennlp) (0.24)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask==1.0.2->allennlp) (7.0)\r\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /opt/conda/lib/python3.6/site-packages (from flask==1.0.2->allennlp) (0.14.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.9.4)\r\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (0.1.7)\r\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.5.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (39.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (18.1.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (4.1.0)\r\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.6.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\r\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\r\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (0.2.9)\r\n",
      "Requirement already satisfied: regex==2018.01.10 in /opt/conda/lib/python3.6/site-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)\r\n",
      "Collecting cookies (from moto==1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 21.1MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from moto==1.3.4->allennlp) (2.2.2)\r\n",
      "Collecting python-jose<3.0.0 (from moto==1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\r\n",
      "Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 24.2MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from moto==1.3.4->allennlp) (2.6.0)\r\n",
      "Collecting pyaml (from moto==1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\r\n",
      "Collecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\r\n",
      "Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/d8/8242b8fb3bd3000274fbf5ac1a06cdba8a5ccbcf4e2a8c05f0ab37999fd8/docker-3.7.1-py2.py3-none-any.whl (134kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 27.4MB/s \r\n",
      "\u001b[?25hCollecting xmltodict (from moto==1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: boto>=2.36.0 in /opt/conda/lib/python3.6/site-packages (from moto==1.3.4->allennlp) (2.48.0)\r\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.6/site-packages (from moto==1.3.4->allennlp) (2.0.0)\r\n",
      "Requirement already satisfied: protobuf>=0.3.2 in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.2->allennlp) (3.7.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==2.2.3->allennlp) (2.2.0)\r\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent==1.3.6->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 20.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: sphinx>=1.2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc==0.8.0->allennlp) (1.7.4)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.10->flask==1.0.2->allennlp) (1.0)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy->allennlp) (0.5.1)\r\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\r\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\r\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (1.10.11)\r\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (0.24.0)\r\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.0.0->moto==1.3.4->allennlp) (1.11.5)\r\n",
      "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 24.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future<1.0 in /opt/conda/lib/python3.6/site-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.17.1)\r\n",
      "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/cf/4b66bf1ac2484ca39599b4576d681186b61b543c2d2c29f9aa4ba3cc53b5/pycryptodome-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 7.5MB 6.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp) (0.9.6)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=2.5.1->moto==1.3.4->allennlp) (0.55.0)\r\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock->moto==1.3.4->allennlp) (5.1.3)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.5.3)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.10)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (17.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.0.0->moto==1.3.4->allennlp) (2.18)\r\n",
      "Building wheels for collected packages: overrides, parsimonious, jsonnet, jsondiff\r\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\r\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/bb/51/82/ae9b22a790f11e7be918939d01aa397c545ebb3723453c5fb4\r\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/a0/aa/f0/b4ab8854cf00f922a87787425cfbb789aac01ab2c2cd1b4ca4\r\n",
      "  Building wheel for jsondiff (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\r\n",
      "Successfully built overrides parsimonious jsonnet jsondiff\r\n",
      "\u001b[31mlime 0.1.1.33 has requirement matplotlib==2.1.0, but you'll have matplotlib 2.2.3 which is incompatible.\u001b[0m\r\n",
      "\u001b[31manaconda-client 1.6.14 has requirement python-dateutil>=2.6.1, but you'll have python-dateutil 2.6.0 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: responses, rsa, botocore, awscli, flaky, overrides, parsimonious, cookies, ecdsa, pycryptodome, python-jose, aws-xray-sdk, pyaml, pytz, jsondiff, docker-pycreds, docker, xmltodict, moto, tensorboardX, matplotlib, jsonnet, sqlparse, editdistance, flask-cors, conllu, greenlet, gevent, allennlp\r\n",
      "  Found existing installation: rsa 4.0\r\n",
      "    Uninstalling rsa-4.0:\r\n",
      "      Successfully uninstalled rsa-4.0\r\n",
      "  Found existing installation: botocore 1.12.118\r\n",
      "    Uninstalling botocore-1.12.118:\r\n",
      "      Successfully uninstalled botocore-1.12.118\r\n",
      "  Found existing installation: pytz 2018.4\r\n",
      "    Uninstalling pytz-2018.4:\r\n",
      "      Successfully uninstalled pytz-2018.4\r\n",
      "  Found existing installation: tensorboardX 1.6\r\n",
      "    Uninstalling tensorboardX-1.6:\r\n",
      "      Successfully uninstalled tensorboardX-1.6\r\n",
      "  Found existing installation: matplotlib 3.0.3\r\n",
      "    Uninstalling matplotlib-3.0.3:\r\n",
      "      Successfully uninstalled matplotlib-3.0.3\r\n",
      "  Found existing installation: Flask-Cors 3.0.4\r\n",
      "    Uninstalling Flask-Cors-3.0.4:\r\n",
      "      Successfully uninstalled Flask-Cors-3.0.4\r\n",
      "Successfully installed allennlp-0.8.2 aws-xray-sdk-0.95 awscli-1.16.130 botocore-1.12.120 conllu-0.11 cookies-2.2.1 docker-3.7.1 docker-pycreds-0.4.0 ecdsa-0.13 editdistance-0.5.3 flaky-3.5.3 flask-cors-3.0.7 gevent-1.3.6 greenlet-0.4.15 jsondiff-1.1.1 jsonnet-0.10.0 matplotlib-2.2.3 moto-1.3.4 overrides-1.9 parsimonious-0.8.0 pyaml-18.11.0 pycryptodome-3.7.3 python-jose-2.0.2 pytz-2017.3 responses-0.10.6 rsa-3.4.2 sqlparse-0.2.4 tensorboardX-1.2 xmltodict-0.12.0\r\n",
      "Collecting https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip\r\n",
      "  Downloading https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip\r\n",
      "\u001b[K     \\ 102kB 6.6MB/s\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from PyTorchHelperBot==0.0.5) (1.0.1.post2)\r\n",
      "Building wheels for collected packages: PyTorchHelperBot\r\n",
      "  Building wheel for PyTorchHelperBot (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-5aw9mdes/wheels/83/9c/cd/789072117df3c77e5fd6368c30b293947b88b92a44034fee3b\r\n",
      "Successfully built PyTorchHelperBot\r\n",
      "Installing collected packages: PyTorchHelperBot\r\n",
      "Successfully installed PyTorchHelperBot-0.0.5\r\n"
     ]
    }
   ],
   "source": [
    "!conda remove -y greenlet\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install allennlp\n",
    "!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "CDcP8jDeAovI"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n",
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n",
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98b461a0d1e2d27558502f9caefeaf7e47871efc",
    "colab_type": "text",
    "id": "9WAyYhZUAovT"
   },
   "source": [
    "\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "4e128e4337fd5c906540c112bc1d4e0fd2f38ef3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D24_hoPlAovh",
    "outputId": "aeffc30a-6828-46ea-b13e-a86de4d81eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# This variable is used by helperbot to make the training deterministic\n",
    "os.environ[\"SEED\"] = \"828\"\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n",
    "\n",
    "from helperbot import (\n",
    "    TriangularLR, BaseBot, WeightDecayOptimizerWrapper,\n",
    "    GradualWarmupScheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4209e502d9d0c58575d71a7580cabc66bbf7ff70",
    "colab": {},
    "colab_type": "code",
    "id": "vlSb_17SAovo"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-large-uncased'\n",
    "CASED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3302e69f42392d81362b53abceb3af54ac95b5d9",
    "colab": {},
    "colab_type": "code",
    "id": "LNfb110WEhXF"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        # self.span_extractor = SelfAttentiveSpanExtractor(bert_hidden_size)\n",
    "        self.span_extractor = EndpointSpanExtractor(\n",
    "            bert_hidden_size, \"x,y,x*y\"\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 7),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(bert_hidden_size * 7, 64),           \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),      \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                print(\"Initing batchnorm\")\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    print(\"Initing linear with weight normalization\")\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    print(\"Initing linear\")\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.size(2) == self.bert_hidden_size\n",
    "        spans_contexts = self.span_extractor(\n",
    "            bert_outputs, \n",
    "            offsets[:, :4].reshape(-1, 2, 2)\n",
    "        ).reshape(offsets.size()[0], -1)\n",
    "        return self.fc(torch.cat([\n",
    "            spans_contexts,\n",
    "            torch.gather(\n",
    "                bert_outputs, 1,\n",
    "                offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n",
    "            ).squeeze(1)\n",
    "        ], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6d3c64ff3a19456ee88ef77825b83690e5907475",
    "colab": {},
    "colab_type": "code",
    "id": "lvTCFyWIAovs"
   },
   "outputs": [],
   "source": [
    "def tokenize(row, tokenizer):\n",
    "    break_points = sorted(\n",
    "        [\n",
    "            (\"A\", row[\"A-offset\"], row[\"A\"]),\n",
    "            (\"B\", row[\"B-offset\"], row[\"B\"]),\n",
    "            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n",
    "        ], key=lambda x: x[0]\n",
    "    )\n",
    "    tokens, spans, current_pos = [], {}, 0\n",
    "    for name, offset, text in break_points:\n",
    "        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "        # Make sure we do not get it wrong\n",
    "        assert row[\"Text\"][offset:offset+len(text)] == text\n",
    "        # Tokenize the target\n",
    "        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n",
    "        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n",
    "        tokens.extend(tmp_tokens)\n",
    "        current_pos = offset + len(text)\n",
    "    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "    assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n",
    "\n",
    "\n",
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            self.y = df.target.values.astype(\"uint8\")\n",
    "        \n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            tokens, offsets = tokenize(row, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None\n",
    "\n",
    "    \n",
    "def collate_examples(batch, truncate_len=490):\n",
    "    \"\"\"Batch preparation.\n",
    "    \n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"    \n",
    "    transposed = list(zip(*batch))\n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    for i, row in enumerate(transposed[0]):\n",
    "        row = np.array(row[:truncate_len])\n",
    "        tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.from_numpy(tokens)\n",
    "    # Offsets\n",
    "    offsets = torch.stack([\n",
    "        torch.LongTensor(x) for x in transposed[1]\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    # Labels\n",
    "    if len(transposed) == 2:\n",
    "        return token_tensor, offsets, None\n",
    "    labels = torch.LongTensor(transposed[2])\n",
    "    return token_tensor, offsets, labels\n",
    "\n",
    "\n",
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device, use_layer: int = -2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.use_layer = use_layer\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=True)\n",
    "        head_outputs = self.head(bert_outputs[self.use_layer], offsets.to(self.device))\n",
    "        return head_outputs            \n",
    "\n",
    "\n",
    "# Adapted from fast.ai library\n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
    "    \n",
    "    \n",
    "class GAPBot(BaseBot):\n",
    "    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n",
    "        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n",
    "        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n",
    "        device=\"cuda:0\", use_tensorboard=False):\n",
    "        super().__init__(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer=optimizer, clip_grad=clip_grad,\n",
    "            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n",
    "            batch_idx=batch_idx, echo=echo,\n",
    "            device=device, use_tensorboard=use_tensorboard\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_format = \"%.6f\"\n",
    "        \n",
    "    def extract_prediction(self, tensor):\n",
    "        return tensor\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n",
    "        loss = self.eval(self.val_loader)\n",
    "        loss_str = self.loss_format % loss\n",
    "        self.logger.info(\"Snapshot loss %s\", loss_str)\n",
    "        self.logger.tb_scalars(\n",
    "            \"losses\", {\"val\": loss},  self.step)\n",
    "        target_path = (\n",
    "            self.checkpoint_dir / \"best.pth\")        \n",
    "        if not self.best_performers or (self.best_performers[0][0] > loss):\n",
    "            torch.save(self.model.state_dict(), target_path)\n",
    "            self.best_performers = [(loss, target_path, self.step)]\n",
    "        self.logger.info(\"Saving checkpoint %s...\", target_path)\n",
    "        assert Path(target_path).exists()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1f78b39f37426ab9a2643c4be09de4b6b466ac84",
    "colab": {},
    "colab_type": "code",
    "id": "TENyTChtCPqi"
   },
   "outputs": [],
   "source": [
    "def extract_target(df):\n",
    "    df[\"Neither\"] = 0\n",
    "    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "    df[\"target\"] = 0\n",
    "    df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "    print(df.target.value_counts())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d534c85ff69192b4dd1ec670fc9c2b9392cc7a62",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "-NT9XHDkAovw",
    "outputId": "bc403ee9-7f7b-4cc5-84fd-f7ed186f9803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1105\n",
      "1    1060\n",
      "2     289\n",
      "Name: target, dtype: int64\n",
      "1    925\n",
      "0    874\n",
      "2    201\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.concat([\n",
    "    pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\"),\n",
    "    pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n",
    "], axis=0)\n",
    "df_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\n",
    "df_train = extract_target(df_train)\n",
    "df_test = extract_target(df_test)\n",
    "sample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\n",
    "assert sample_sub.shape[0] == df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d58d18c34f5df9ec8f8d8fb048ae6c10fbf9914a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qu8XkjmNAov0",
    "outputId": "6fadb239-d6ca-4006-8f95-d31ec4414356"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 5771091.08B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    BERT_MODEL,\n",
    "    do_lower_case=CASED,\n",
    "    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "69689365738454b33649a14d83eea49cc1b18687",
    "colab": {},
    "colab_type": "code",
    "id": "z-Wx2NdAAov5"
   },
   "outputs": [],
   "source": [
    "test_ds = GAPDataset(df_test, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "391f4f961ee70ac7b2731a04afe9d45e5458d82f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11132
    },
    "colab_type": "code",
    "id": "3fn1H8nFBg2m",
    "outputId": "14ee0705-9440-4e79-a2a6-de9e6c50c95d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248501532/1248501532 [00:29<00:00, 41939019.50B/s]\n",
      "[[03/23/2019 04:34:11 PM]] SEED: 828\n",
      "[[03/23/2019 04:34:11 PM]] # of paramters: 335,615,363\n",
      "[[03/23/2019 04:34:11 PM]] # of trainable paramters: 473,475\n",
      "[[03/23/2019 04:34:11 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/23/2019 04:34:11 PM]] Batches per epoch: 61\n",
      "[[03/23/2019 04:34:11 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 04:34:26 PM]] Step 30: train 1.603552 lr: 3.333e-04\n",
      "[[03/23/2019 04:34:40 PM]] Step 60: train 1.388293 lr: 5.833e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.11s/it]\n",
      "[[03/23/2019 04:34:49 PM]] Snapshot loss 0.810265\n",
      "[[03/23/2019 04:34:51 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:34:51 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:34:51 PM]] ====================Epoch 2====================\n",
      "[[03/23/2019 04:35:06 PM]] Step 90: train 1.238364 lr: 8.333e-04\n",
      "[[03/23/2019 04:35:19 PM]] Step 120: train 1.171656 lr: 1.083e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:35:29 PM]] Snapshot loss 0.707478\n",
      "[[03/23/2019 04:35:31 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:35:31 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:35:31 PM]] ====================Epoch 3====================\n",
      "[[03/23/2019 04:35:45 PM]] Step 150: train 1.078989 lr: 1.333e-03\n",
      "[[03/23/2019 04:35:59 PM]] Step 180: train 1.025751 lr: 1.583e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:36:09 PM]] Snapshot loss 0.695795\n",
      "[[03/23/2019 04:36:11 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:36:11 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:36:11 PM]] ====================Epoch 4====================\n",
      "[[03/23/2019 04:36:24 PM]] Step 210: train 0.972197 lr: 1.833e-03\n",
      "[[03/23/2019 04:36:38 PM]] Step 240: train 0.933568 lr: 1.972e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:36:49 PM]] Snapshot loss 0.620209\n",
      "[[03/23/2019 04:36:51 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:36:51 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:36:51 PM]] ====================Epoch 5====================\n",
      "[[03/23/2019 04:37:03 PM]] Step 270: train 0.896164 lr: 1.889e-03\n",
      "[[03/23/2019 04:37:18 PM]] Step 300: train 0.864690 lr: 1.806e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:37:29 PM]] Snapshot loss 0.613803\n",
      "[[03/23/2019 04:37:31 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:37:31 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:37:31 PM]] ====================Epoch 6====================\n",
      "[[03/23/2019 04:37:43 PM]] Step 330: train 0.752697 lr: 1.723e-03\n",
      "[[03/23/2019 04:37:57 PM]] Step 360: train 0.685057 lr: 1.640e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:38:09 PM]] Snapshot loss 0.618312\n",
      "[[03/23/2019 04:38:09 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:38:09 PM]] ====================Epoch 7====================\n",
      "[[03/23/2019 04:38:20 PM]] Step 390: train 0.640325 lr: 1.557e-03\n",
      "[[03/23/2019 04:38:35 PM]] Step 420: train 0.587627 lr: 1.475e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:38:47 PM]] Snapshot loss 0.610482\n",
      "[[03/23/2019 04:38:49 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:38:49 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:38:49 PM]] ====================Epoch 8====================\n",
      "[[03/23/2019 04:39:00 PM]] Step 450: train 0.561742 lr: 1.392e-03\n",
      "[[03/23/2019 04:39:15 PM]] Step 480: train 0.531832 lr: 1.309e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:39:27 PM]] Snapshot loss 0.607771\n",
      "[[03/23/2019 04:39:29 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:39:29 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:39:29 PM]] ====================Epoch 9====================\n",
      "[[03/23/2019 04:39:39 PM]] Step 510: train 0.505651 lr: 1.226e-03\n",
      "[[03/23/2019 04:39:54 PM]] Step 540: train 0.478503 lr: 1.143e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:40:07 PM]] Snapshot loss 0.610125\n",
      "[[03/23/2019 04:40:07 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:40:07 PM]] ====================Epoch 10====================\n",
      "[[03/23/2019 04:40:17 PM]] Step 570: train 0.452774 lr: 1.060e-03\n",
      "[[03/23/2019 04:40:31 PM]] Step 600: train 0.432140 lr: 9.767e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:40:45 PM]] Snapshot loss 0.605936\n",
      "[[03/23/2019 04:40:47 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:40:47 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:40:47 PM]] ====================Epoch 11====================\n",
      "[[03/23/2019 04:40:57 PM]] Step 630: train 0.415895 lr: 8.937e-04\n",
      "[[03/23/2019 04:41:10 PM]] Step 660: train 0.401497 lr: 8.108e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:41:24 PM]] Snapshot loss 0.615185\n",
      "[[03/23/2019 04:41:24 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:41:24 PM]] ====================Epoch 12====================\n",
      "[[03/23/2019 04:41:34 PM]] Step 690: train 0.389036 lr: 7.278e-04\n",
      "[[03/23/2019 04:41:49 PM]] Step 720: train 0.378354 lr: 6.448e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:42:02 PM]] Snapshot loss 0.606232\n",
      "[[03/23/2019 04:42:02 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:42:02 PM]] ====================Epoch 13====================\n",
      "[[03/23/2019 04:42:11 PM]] Step 750: train 0.366488 lr: 5.619e-04\n",
      "[[03/23/2019 04:42:25 PM]] Step 780: train 0.355430 lr: 4.789e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:42:40 PM]] Snapshot loss 0.611732\n",
      "[[03/23/2019 04:42:40 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:42:40 PM]] ====================Epoch 14====================\n",
      "[[03/23/2019 04:42:48 PM]] Step 810: train 0.345842 lr: 3.959e-04\n",
      "[[03/23/2019 04:43:02 PM]] Step 840: train 0.338850 lr: 3.130e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:43:19 PM]] Snapshot loss 0.614103\n",
      "[[03/23/2019 04:43:19 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:43:19 PM]] ====================Epoch 15====================\n",
      "[[03/23/2019 04:43:26 PM]] Step 870: train 0.333671 lr: 2.300e-04\n",
      "[[03/23/2019 04:43:41 PM]] Step 900: train 0.327220 lr: 1.470e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n",
      "[[03/23/2019 04:43:56 PM]] Snapshot loss 0.603512\n",
      "[[03/23/2019 04:43:58 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:43:59 PM]] New low\n",
      "\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.11s/it]\n",
      "[[03/23/2019 04:44:08 PM]] Confirm val loss: 0.6035\n",
      "100%|██████████| 16/16 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 2\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 04:45:13 PM]] SEED: 828\n",
      "[[03/23/2019 04:45:13 PM]] # of paramters: 335,615,363\n",
      "[[03/23/2019 04:45:13 PM]] # of trainable paramters: 473,475\n",
      "[[03/23/2019 04:45:13 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/23/2019 04:45:13 PM]] Batches per epoch: 61\n",
      "[[03/23/2019 04:45:13 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 04:45:27 PM]] Step 30: train 1.562702 lr: 3.333e-04\n",
      "[[03/23/2019 04:45:42 PM]] Step 60: train 1.404720 lr: 5.833e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:45:52 PM]] Snapshot loss 0.776403\n",
      "[[03/23/2019 04:45:54 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:45:54 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:45:54 PM]] ====================Epoch 2====================\n",
      "[[03/23/2019 04:46:08 PM]] Step 90: train 1.258263 lr: 8.333e-04\n",
      "[[03/23/2019 04:46:23 PM]] Step 120: train 1.174511 lr: 1.083e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:46:32 PM]] Snapshot loss 0.748883\n",
      "[[03/23/2019 04:46:34 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:46:34 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:46:35 PM]] ====================Epoch 3====================\n",
      "[[03/23/2019 04:46:48 PM]] Step 150: train 1.089663 lr: 1.333e-03\n",
      "[[03/23/2019 04:47:03 PM]] Step 180: train 1.019355 lr: 1.583e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.35s/it]\n",
      "[[03/23/2019 04:47:14 PM]] Snapshot loss 0.652177\n",
      "[[03/23/2019 04:47:16 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:47:16 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:47:16 PM]] ====================Epoch 4====================\n",
      "[[03/23/2019 04:47:29 PM]] Step 210: train 0.968060 lr: 1.833e-03\n",
      "[[03/23/2019 04:47:44 PM]] Step 240: train 0.928671 lr: 1.972e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.35s/it]\n",
      "[[03/23/2019 04:47:55 PM]] Snapshot loss 0.644124\n",
      "[[03/23/2019 04:47:57 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:47:57 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:47:57 PM]] ====================Epoch 5====================\n",
      "[[03/23/2019 04:48:10 PM]] Step 270: train 0.890860 lr: 1.889e-03\n",
      "[[03/23/2019 04:48:24 PM]] Step 300: train 0.859161 lr: 1.806e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:48:36 PM]] Snapshot loss 0.572326\n",
      "[[03/23/2019 04:48:38 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:48:38 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:48:38 PM]] ====================Epoch 6====================\n",
      "[[03/23/2019 04:48:51 PM]] Step 330: train 0.754990 lr: 1.723e-03\n",
      "[[03/23/2019 04:49:05 PM]] Step 360: train 0.677107 lr: 1.640e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:49:16 PM]] Snapshot loss 0.567069\n",
      "[[03/23/2019 04:49:19 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:49:19 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:49:19 PM]] ====================Epoch 7====================\n",
      "[[03/23/2019 04:49:30 PM]] Step 390: train 0.623200 lr: 1.557e-03\n",
      "[[03/23/2019 04:49:45 PM]] Step 420: train 0.582341 lr: 1.475e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:49:57 PM]] Snapshot loss 0.565665\n",
      "[[03/23/2019 04:50:00 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:50:00 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:50:00 PM]] ====================Epoch 8====================\n",
      "[[03/23/2019 04:50:11 PM]] Step 450: train 0.550265 lr: 1.392e-03\n",
      "[[03/23/2019 04:50:25 PM]] Step 480: train 0.525515 lr: 1.309e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:50:38 PM]] Snapshot loss 0.581797\n",
      "[[03/23/2019 04:50:38 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:50:38 PM]] ====================Epoch 9====================\n",
      "[[03/23/2019 04:50:49 PM]] Step 510: train 0.499594 lr: 1.226e-03\n",
      "[[03/23/2019 04:51:02 PM]] Step 540: train 0.472857 lr: 1.143e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:51:16 PM]] Snapshot loss 0.581056\n",
      "[[03/23/2019 04:51:16 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:51:17 PM]] ====================Epoch 10====================\n",
      "[[03/23/2019 04:51:27 PM]] Step 570: train 0.452167 lr: 1.060e-03\n",
      "[[03/23/2019 04:51:41 PM]] Step 600: train 0.430777 lr: 9.767e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.35s/it]\n",
      "[[03/23/2019 04:51:55 PM]] Snapshot loss 0.582628\n",
      "[[03/23/2019 04:51:55 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:51:55 PM]] ====================Epoch 11====================\n",
      "[[03/23/2019 04:52:04 PM]] Step 630: train 0.414372 lr: 8.937e-04\n",
      "[[03/23/2019 04:52:20 PM]] Step 660: train 0.404370 lr: 8.108e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.35s/it]\n",
      "[[03/23/2019 04:52:34 PM]] Snapshot loss 0.595881\n",
      "[[03/23/2019 04:52:34 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:52:34 PM]] ====================Epoch 12====================\n",
      "[[03/23/2019 04:52:43 PM]] Step 690: train 0.395751 lr: 7.278e-04\n",
      "[[03/23/2019 04:52:58 PM]] Step 720: train 0.378689 lr: 6.448e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:53:12 PM]] Snapshot loss 0.598224\n",
      "[[03/23/2019 04:53:12 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:53:12 PM]] ====================Epoch 13====================\n",
      "[[03/23/2019 04:53:21 PM]] Step 750: train 0.367196 lr: 5.619e-04\n",
      "[[03/23/2019 04:53:36 PM]] Step 780: train 0.357698 lr: 4.789e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:53:51 PM]] Snapshot loss 0.605937\n",
      "[[03/23/2019 04:53:51 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:53:51 PM]] ====================Epoch 14====================\n",
      "[[03/23/2019 04:54:00 PM]] Step 810: train 0.352852 lr: 3.959e-04\n",
      "[[03/23/2019 04:54:14 PM]] Step 840: train 0.345983 lr: 3.130e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:54:30 PM]] Snapshot loss 0.610567\n",
      "[[03/23/2019 04:54:30 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:54:30 PM]] ====================Epoch 15====================\n",
      "[[03/23/2019 04:54:37 PM]] Step 870: train 0.337632 lr: 2.300e-04\n",
      "[[03/23/2019 04:54:52 PM]] Step 900: train 0.329029 lr: 1.470e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.35s/it]\n",
      "[[03/23/2019 04:55:08 PM]] Snapshot loss 0.601731\n",
      "[[03/23/2019 04:55:08 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.34s/it]\n",
      "[[03/23/2019 04:55:19 PM]] Confirm val loss: 0.5657\n",
      "100%|██████████| 16/16 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 3\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 04:56:23 PM]] SEED: 828\n",
      "[[03/23/2019 04:56:23 PM]] # of paramters: 335,615,363\n",
      "[[03/23/2019 04:56:23 PM]] # of trainable paramters: 473,475\n",
      "[[03/23/2019 04:56:23 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/23/2019 04:56:23 PM]] Batches per epoch: 61\n",
      "[[03/23/2019 04:56:23 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 04:56:38 PM]] Step 30: train 1.584015 lr: 3.333e-04\n",
      "[[03/23/2019 04:56:52 PM]] Step 60: train 1.377953 lr: 5.833e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 04:57:01 PM]] Snapshot loss 0.807589\n",
      "[[03/23/2019 04:57:03 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:57:03 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:57:03 PM]] ====================Epoch 2====================\n",
      "[[03/23/2019 04:57:18 PM]] Step 90: train 1.235471 lr: 8.333e-04\n",
      "[[03/23/2019 04:57:32 PM]] Step 120: train 1.146436 lr: 1.083e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 04:57:43 PM]] Snapshot loss 0.671906\n",
      "[[03/23/2019 04:57:45 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:57:45 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:57:45 PM]] ====================Epoch 3====================\n",
      "[[03/23/2019 04:57:59 PM]] Step 150: train 1.068223 lr: 1.333e-03\n",
      "[[03/23/2019 04:58:14 PM]] Step 180: train 1.015130 lr: 1.583e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 04:58:24 PM]] Snapshot loss 0.637062\n",
      "[[03/23/2019 04:58:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:58:27 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:58:27 PM]] ====================Epoch 4====================\n",
      "[[03/23/2019 04:58:40 PM]] Step 210: train 0.967452 lr: 1.833e-03\n",
      "[[03/23/2019 04:58:54 PM]] Step 240: train 0.922825 lr: 1.972e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 04:59:06 PM]] Snapshot loss 0.629429\n",
      "[[03/23/2019 04:59:08 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:59:08 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:59:08 PM]] ====================Epoch 5====================\n",
      "[[03/23/2019 04:59:20 PM]] Step 270: train 0.885397 lr: 1.889e-03\n",
      "[[03/23/2019 04:59:35 PM]] Step 300: train 0.853785 lr: 1.806e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 04:59:47 PM]] Snapshot loss 0.584178\n",
      "[[03/23/2019 04:59:49 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 04:59:49 PM]] New low\n",
      "\n",
      "[[03/23/2019 04:59:49 PM]] ====================Epoch 6====================\n",
      "[[03/23/2019 05:00:01 PM]] Step 330: train 0.747855 lr: 1.723e-03\n",
      "[[03/23/2019 05:00:15 PM]] Step 360: train 0.680466 lr: 1.640e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:00:27 PM]] Snapshot loss 0.574293\n",
      "[[03/23/2019 05:00:29 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:00:29 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:00:29 PM]] ====================Epoch 7====================\n",
      "[[03/23/2019 05:00:41 PM]] Step 390: train 0.633289 lr: 1.557e-03\n",
      "[[03/23/2019 05:00:56 PM]] Step 420: train 0.593076 lr: 1.475e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:01:08 PM]] Snapshot loss 0.584336\n",
      "[[03/23/2019 05:01:08 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:01:08 PM]] ====================Epoch 8====================\n",
      "[[03/23/2019 05:01:19 PM]] Step 450: train 0.558016 lr: 1.392e-03\n",
      "[[03/23/2019 05:01:34 PM]] Step 480: train 0.524679 lr: 1.309e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:01:47 PM]] Snapshot loss 0.601969\n",
      "[[03/23/2019 05:01:47 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:01:47 PM]] ====================Epoch 9====================\n",
      "[[03/23/2019 05:01:58 PM]] Step 510: train 0.494005 lr: 1.226e-03\n",
      "[[03/23/2019 05:02:13 PM]] Step 540: train 0.471509 lr: 1.143e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:02:25 PM]] Snapshot loss 0.593088\n",
      "[[03/23/2019 05:02:25 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:02:25 PM]] ====================Epoch 10====================\n",
      "[[03/23/2019 05:02:36 PM]] Step 570: train 0.449937 lr: 1.060e-03\n",
      "[[03/23/2019 05:02:51 PM]] Step 600: train 0.433401 lr: 9.767e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:03:04 PM]] Snapshot loss 0.586346\n",
      "[[03/23/2019 05:03:04 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:03:05 PM]] ====================Epoch 11====================\n",
      "[[03/23/2019 05:03:14 PM]] Step 630: train 0.413947 lr: 8.937e-04\n",
      "[[03/23/2019 05:03:28 PM]] Step 660: train 0.401252 lr: 8.108e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 05:03:43 PM]] Snapshot loss 0.592921\n",
      "[[03/23/2019 05:03:43 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:03:43 PM]] ====================Epoch 12====================\n",
      "[[03/23/2019 05:03:52 PM]] Step 690: train 0.385956 lr: 7.278e-04\n",
      "[[03/23/2019 05:04:06 PM]] Step 720: train 0.372378 lr: 6.448e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:04:22 PM]] Snapshot loss 0.585918\n",
      "[[03/23/2019 05:04:22 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:04:22 PM]] ====================Epoch 13====================\n",
      "[[03/23/2019 05:04:31 PM]] Step 750: train 0.364851 lr: 5.619e-04\n",
      "[[03/23/2019 05:04:46 PM]] Step 780: train 0.353640 lr: 4.789e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:05:01 PM]] Snapshot loss 0.591761\n",
      "[[03/23/2019 05:05:01 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:05:01 PM]] ====================Epoch 14====================\n",
      "[[03/23/2019 05:05:09 PM]] Step 810: train 0.348385 lr: 3.959e-04\n",
      "[[03/23/2019 05:05:23 PM]] Step 840: train 0.339411 lr: 3.130e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 05:05:40 PM]] Snapshot loss 0.588329\n",
      "[[03/23/2019 05:05:40 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:05:40 PM]] ====================Epoch 15====================\n",
      "[[03/23/2019 05:05:47 PM]] Step 870: train 0.331488 lr: 2.300e-04\n",
      "[[03/23/2019 05:06:03 PM]] Step 900: train 0.316376 lr: 1.470e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.22s/it]\n",
      "[[03/23/2019 05:06:19 PM]] Snapshot loss 0.596532\n",
      "[[03/23/2019 05:06:19 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "[[03/23/2019 05:06:29 PM]] Confirm val loss: 0.5743\n",
      "100%|██████████| 16/16 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 4\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 05:07:33 PM]] SEED: 828\n",
      "[[03/23/2019 05:07:33 PM]] # of paramters: 335,615,363\n",
      "[[03/23/2019 05:07:33 PM]] # of trainable paramters: 473,475\n",
      "[[03/23/2019 05:07:34 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/23/2019 05:07:34 PM]] Batches per epoch: 61\n",
      "[[03/23/2019 05:07:34 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 05:07:48 PM]] Step 30: train 1.788291 lr: 3.333e-04\n",
      "[[03/23/2019 05:08:03 PM]] Step 60: train 1.546251 lr: 5.833e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:08:12 PM]] Snapshot loss 0.847641\n",
      "[[03/23/2019 05:08:14 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:08:14 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:08:14 PM]] ====================Epoch 2====================\n",
      "[[03/23/2019 05:08:28 PM]] Step 90: train 1.369124 lr: 8.333e-04\n",
      "[[03/23/2019 05:08:43 PM]] Step 120: train 1.276816 lr: 1.083e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:08:52 PM]] Snapshot loss 0.743962\n",
      "[[03/23/2019 05:08:55 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:08:55 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:08:55 PM]] ====================Epoch 3====================\n",
      "[[03/23/2019 05:09:09 PM]] Step 150: train 1.205539 lr: 1.333e-03\n",
      "[[03/23/2019 05:09:24 PM]] Step 180: train 1.146154 lr: 1.583e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:09:33 PM]] Snapshot loss 0.633747\n",
      "[[03/23/2019 05:09:35 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:09:35 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:09:36 PM]] ====================Epoch 4====================\n",
      "[[03/23/2019 05:09:49 PM]] Step 210: train 1.080850 lr: 1.833e-03\n",
      "[[03/23/2019 05:10:04 PM]] Step 240: train 1.028109 lr: 1.972e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:10:14 PM]] Snapshot loss 0.662319\n",
      "[[03/23/2019 05:10:14 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:10:14 PM]] ====================Epoch 5====================\n",
      "[[03/23/2019 05:10:27 PM]] Step 270: train 0.981486 lr: 1.889e-03\n",
      "[[03/23/2019 05:10:42 PM]] Step 300: train 0.942832 lr: 1.806e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:10:53 PM]] Snapshot loss 0.611829\n",
      "[[03/23/2019 05:10:55 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:10:55 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:10:55 PM]] ====================Epoch 6====================\n",
      "[[03/23/2019 05:11:08 PM]] Step 330: train 0.814294 lr: 1.723e-03\n",
      "[[03/23/2019 05:11:23 PM]] Step 360: train 0.733588 lr: 1.640e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:11:34 PM]] Snapshot loss 0.573382\n",
      "[[03/23/2019 05:11:36 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:11:36 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:11:36 PM]] ====================Epoch 7====================\n",
      "[[03/23/2019 05:11:48 PM]] Step 390: train 0.676629 lr: 1.557e-03\n",
      "[[03/23/2019 05:12:03 PM]] Step 420: train 0.620822 lr: 1.475e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:12:15 PM]] Snapshot loss 0.613527\n",
      "[[03/23/2019 05:12:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:12:15 PM]] ====================Epoch 8====================\n",
      "[[03/23/2019 05:12:25 PM]] Step 450: train 0.568816 lr: 1.392e-03\n",
      "[[03/23/2019 05:12:40 PM]] Step 480: train 0.526886 lr: 1.309e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:12:53 PM]] Snapshot loss 0.629686\n",
      "[[03/23/2019 05:12:53 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:12:53 PM]] ====================Epoch 9====================\n",
      "[[03/23/2019 05:13:03 PM]] Step 510: train 0.500315 lr: 1.226e-03\n",
      "[[03/23/2019 05:13:19 PM]] Step 540: train 0.474074 lr: 1.143e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.97s/it]\n",
      "[[03/23/2019 05:13:31 PM]] Snapshot loss 0.579373\n",
      "[[03/23/2019 05:13:31 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:13:31 PM]] ====================Epoch 10====================\n",
      "[[03/23/2019 05:13:42 PM]] Step 570: train 0.451757 lr: 1.060e-03\n",
      "[[03/23/2019 05:13:56 PM]] Step 600: train 0.427468 lr: 9.767e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:14:10 PM]] Snapshot loss 0.588544\n",
      "[[03/23/2019 05:14:10 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:14:10 PM]] ====================Epoch 11====================\n",
      "[[03/23/2019 05:14:20 PM]] Step 630: train 0.414014 lr: 8.937e-04\n",
      "[[03/23/2019 05:14:35 PM]] Step 660: train 0.400523 lr: 8.108e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:14:48 PM]] Snapshot loss 0.609550\n",
      "[[03/23/2019 05:14:48 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:14:48 PM]] ====================Epoch 12====================\n",
      "[[03/23/2019 05:14:58 PM]] Step 690: train 0.391681 lr: 7.278e-04\n",
      "[[03/23/2019 05:15:12 PM]] Step 720: train 0.383649 lr: 6.448e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:15:26 PM]] Snapshot loss 0.605503\n",
      "[[03/23/2019 05:15:26 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:15:26 PM]] ====================Epoch 13====================\n",
      "[[03/23/2019 05:15:35 PM]] Step 750: train 0.376410 lr: 5.619e-04\n",
      "[[03/23/2019 05:15:50 PM]] Step 780: train 0.365919 lr: 4.789e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:16:05 PM]] Snapshot loss 0.603794\n",
      "[[03/23/2019 05:16:05 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:16:05 PM]] ====================Epoch 14====================\n",
      "[[03/23/2019 05:16:13 PM]] Step 810: train 0.354042 lr: 3.959e-04\n",
      "[[03/23/2019 05:16:27 PM]] Step 840: train 0.343512 lr: 3.130e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:16:43 PM]] Snapshot loss 0.604332\n",
      "[[03/23/2019 05:16:43 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:16:43 PM]] ====================Epoch 15====================\n",
      "[[03/23/2019 05:16:51 PM]] Step 870: train 0.332078 lr: 2.300e-04\n",
      "[[03/23/2019 05:17:05 PM]] Step 900: train 0.323229 lr: 1.470e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.97s/it]\n",
      "[[03/23/2019 05:17:21 PM]] Snapshot loss 0.598406\n",
      "[[03/23/2019 05:17:21 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 4/4 [00:08<00:00,  1.98s/it]\n",
      "[[03/23/2019 05:17:31 PM]] Confirm val loss: 0.5734\n",
      "100%|██████████| 16/16 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 5\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 05:18:35 PM]] SEED: 828\n",
      "[[03/23/2019 05:18:35 PM]] # of paramters: 335,615,363\n",
      "[[03/23/2019 05:18:35 PM]] # of trainable paramters: 473,475\n",
      "[[03/23/2019 05:18:35 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/23/2019 05:18:35 PM]] Batches per epoch: 61\n",
      "[[03/23/2019 05:18:35 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/23/2019 05:18:49 PM]] Step 30: train 1.677634 lr: 3.333e-04\n",
      "[[03/23/2019 05:19:04 PM]] Step 60: train 1.435227 lr: 5.833e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:19:14 PM]] Snapshot loss 0.927675\n",
      "[[03/23/2019 05:19:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:19:15 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:19:15 PM]] ====================Epoch 2====================\n",
      "[[03/23/2019 05:19:30 PM]] Step 90: train 1.326914 lr: 8.333e-04\n",
      "[[03/23/2019 05:19:44 PM]] Step 120: train 1.242616 lr: 1.083e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:19:54 PM]] Snapshot loss 0.701232\n",
      "[[03/23/2019 05:19:56 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:19:56 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:19:56 PM]] ====================Epoch 3====================\n",
      "[[03/23/2019 05:20:10 PM]] Step 150: train 1.148517 lr: 1.333e-03\n",
      "[[03/23/2019 05:20:24 PM]] Step 180: train 1.077824 lr: 1.583e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:20:35 PM]] Snapshot loss 0.667533\n",
      "[[03/23/2019 05:20:37 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:20:37 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:20:37 PM]] ====================Epoch 4====================\n",
      "[[03/23/2019 05:20:50 PM]] Step 210: train 1.025474 lr: 1.833e-03\n",
      "[[03/23/2019 05:21:04 PM]] Step 240: train 0.979424 lr: 1.972e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:21:15 PM]] Snapshot loss 0.661990\n",
      "[[03/23/2019 05:21:17 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:21:17 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:21:17 PM]] ====================Epoch 5====================\n",
      "[[03/23/2019 05:21:31 PM]] Step 270: train 0.933240 lr: 1.889e-03\n",
      "[[03/23/2019 05:21:45 PM]] Step 300: train 0.897076 lr: 1.806e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:21:56 PM]] Snapshot loss 0.593413\n",
      "[[03/23/2019 05:21:58 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:21:58 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:21:59 PM]] ====================Epoch 6====================\n",
      "[[03/23/2019 05:22:11 PM]] Step 330: train 0.785136 lr: 1.723e-03\n",
      "[[03/23/2019 05:22:25 PM]] Step 360: train 0.719002 lr: 1.640e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:22:37 PM]] Snapshot loss 0.569791\n",
      "[[03/23/2019 05:22:39 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:22:39 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:22:39 PM]] ====================Epoch 7====================\n",
      "[[03/23/2019 05:22:51 PM]] Step 390: train 0.655953 lr: 1.557e-03\n",
      "[[03/23/2019 05:23:05 PM]] Step 420: train 0.604894 lr: 1.475e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:23:18 PM]] Snapshot loss 0.566727\n",
      "[[03/23/2019 05:23:20 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:23:20 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:23:20 PM]] ====================Epoch 8====================\n",
      "[[03/23/2019 05:23:30 PM]] Step 450: train 0.572662 lr: 1.392e-03\n",
      "[[03/23/2019 05:23:46 PM]] Step 480: train 0.541285 lr: 1.309e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:23:59 PM]] Snapshot loss 0.570333\n",
      "[[03/23/2019 05:23:59 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:23:59 PM]] ====================Epoch 9====================\n",
      "[[03/23/2019 05:24:10 PM]] Step 510: train 0.510921 lr: 1.226e-03\n",
      "[[03/23/2019 05:24:24 PM]] Step 540: train 0.480880 lr: 1.143e-03\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.12s/it]\n",
      "[[03/23/2019 05:24:37 PM]] Snapshot loss 0.568277\n",
      "[[03/23/2019 05:24:37 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:24:38 PM]] ====================Epoch 10====================\n",
      "[[03/23/2019 05:24:48 PM]] Step 570: train 0.461771 lr: 1.060e-03\n",
      "[[03/23/2019 05:25:02 PM]] Step 600: train 0.444250 lr: 9.767e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:25:16 PM]] Snapshot loss 0.581869\n",
      "[[03/23/2019 05:25:16 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:25:16 PM]] ====================Epoch 11====================\n",
      "[[03/23/2019 05:25:26 PM]] Step 630: train 0.426964 lr: 8.937e-04\n",
      "[[03/23/2019 05:25:41 PM]] Step 660: train 0.410563 lr: 8.108e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:25:55 PM]] Snapshot loss 0.571199\n",
      "[[03/23/2019 05:25:55 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:25:55 PM]] ====================Epoch 12====================\n",
      "[[03/23/2019 05:26:04 PM]] Step 690: train 0.395287 lr: 7.278e-04\n",
      "[[03/23/2019 05:26:19 PM]] Step 720: train 0.379376 lr: 6.448e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:26:33 PM]] Snapshot loss 0.565690\n",
      "[[03/23/2019 05:26:35 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:26:35 PM]] New low\n",
      "\n",
      "[[03/23/2019 05:26:35 PM]] ====================Epoch 13====================\n",
      "[[03/23/2019 05:26:44 PM]] Step 750: train 0.367235 lr: 5.619e-04\n",
      "[[03/23/2019 05:26:58 PM]] Step 780: train 0.359392 lr: 4.789e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:27:13 PM]] Snapshot loss 0.571023\n",
      "[[03/23/2019 05:27:13 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:27:13 PM]] ====================Epoch 14====================\n",
      "[[03/23/2019 05:27:21 PM]] Step 810: train 0.352969 lr: 3.959e-04\n",
      "[[03/23/2019 05:27:36 PM]] Step 840: train 0.347311 lr: 3.130e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:27:52 PM]] Snapshot loss 0.574036\n",
      "[[03/23/2019 05:27:52 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/23/2019 05:27:52 PM]] ====================Epoch 15====================\n",
      "[[03/23/2019 05:28:00 PM]] Step 870: train 0.338785 lr: 2.300e-04\n",
      "[[03/23/2019 05:28:15 PM]] Step 900: train 0.329799 lr: 1.470e-04\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.12s/it]\n",
      "[[03/23/2019 05:28:31 PM]] Snapshot loss 0.568389\n",
      "[[03/23/2019 05:28:31 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "[[03/23/2019 05:28:41 PM]] Confirm val loss: 0.5657\n",
      "100%|██████████| 16/16 [00:35<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=191)\n",
    "\n",
    "val_preds, test_preds, val_ys, val_losses = [], [], [], []\n",
    "for train_index, valid_index in skf.split(df_train, df_train[\"target\"]):\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Fold {len(val_preds) + 1}\")\n",
    "    print(\"=\" * 20)\n",
    "    train_ds = GAPDataset(df_train.iloc[train_index], tokenizer)\n",
    "    val_ds = GAPDataset(df_train.iloc[valid_index], tokenizer)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n",
    "    # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n",
    "    set_trainable(model.bert, False)\n",
    "    set_trainable(model.head, True)\n",
    "    optimizer = WeightDecayOptimizerWrapper(\n",
    "        torch.optim.Adam(model.parameters(), lr=2e-3),\n",
    "        0.05\n",
    "    )\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    bot = GAPBot(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer=optimizer, echo=True,\n",
    "        avg_window=40\n",
    "    )\n",
    "    gc.collect()\n",
    "    steps_per_epoch = len(train_loader) \n",
    "    n_steps = steps_per_epoch * 15\n",
    "    bot.train(\n",
    "        n_steps,\n",
    "        log_interval=steps_per_epoch // 2,\n",
    "        snapshot_interval=steps_per_epoch,\n",
    "#         scheduler=GradualWarmupScheduler(optimizer, 20, int(steps_per_epoch * 4),\n",
    "#             after_scheduler=CosineAnnealingLR(\n",
    "#                 optimizer, n_steps - int(steps_per_epoch * 4)\n",
    "#             )\n",
    "#         )\n",
    "        scheduler=TriangularLR(\n",
    "            optimizer, 20, ratio=3, steps_per_cycle=n_steps)\n",
    "    )\n",
    "    # Load the best checkpoint\n",
    "    bot.load_model(bot.best_performers[0][1])\n",
    "    bot.remove_checkpoints(keep=0)    \n",
    "    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
    "    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n",
    "    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n",
    "    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n",
    "    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4964c9bdcae7cdd5db2d8878d534a4fa7c554be1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "9Qd6IyQ-w11i",
    "outputId": "0947f722-d2ce-403d-aac6-3b66fbee62f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6035136767460817,\n",
       " 0.5656657646229498,\n",
       " 0.5742931585171395,\n",
       " 0.5733822381570001,\n",
       " 0.5656916953703479]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "95a30126727db3ee19e9469d563196332ca0f815",
    "colab": {},
    "colab_type": "code",
    "id": "tptt3GhxWQWC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_preds = np.mean(test_preds, axis=0)\n",
    "final_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8fb035aaaf7d4908067360984619abaf25ee7581",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sIdoG3PuWqhK",
    "outputId": "b11277cb-6003-4ef3-e75e-e05b2fb79e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5227941005793401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(df_test.target, final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "51b7e2335c8f9c1b821b6aeaba7c5122fe74530a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "VFeDfICvAowc",
    "outputId": "07da31ee-bce0-46d4-c745-35a101c4b9df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.619025</td>\n",
       "      <td>0.229578</td>\n",
       "      <td>0.151396</td>\n",
       "      <td>development-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990982</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>development-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.804334</td>\n",
       "      <td>0.178818</td>\n",
       "      <td>development-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022718</td>\n",
       "      <td>0.599286</td>\n",
       "      <td>0.377996</td>\n",
       "      <td>development-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.973471</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>development-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B   NEITHER             ID\n",
       "0  0.619025  0.229578  0.151396  development-1\n",
       "1  0.990982  0.004994  0.004023  development-2\n",
       "2  0.016847  0.804334  0.178818  development-3\n",
       "3  0.022718  0.599286  0.377996  development-4\n",
       "4  0.012704  0.973471  0.013825  development-5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission file\n",
    "df_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\n",
    "df_sub[\"ID\"] = df_test.ID\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "de3e083756ce073f5325ea224424c1da57346321",
    "colab": {},
    "colab_type": "code",
    "id": "uVdAPJUQ3sSH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kernel - span.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
